{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce4b3d7-8d4d-48af-b4d0-787c3e7764f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sray.network.mask2former.mask2former import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e01ea7-2ff3-4bea-b31f-5419c63b7cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6fff113-dfa4-4b65-8a91-07cfbb8b7611",
   "metadata": {},
   "outputs": [],
   "source": [
    "scannet_dir = 'data/scannet'\n",
    "img_files = glob.glob(os.path.join(scannet_dir,\"*\",\"color\",\"*.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1993a4d-0f3e-44a5-9a6a-64148eff8d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/scannet/scene0011_00/color/637.jpg'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4770572c-2af8-417d-accb-77252e70512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_id_from_img_path(img_path):\n",
    "    return img_path.split('/')[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1020a7e8-2b87-45d0-9a8b-059a3773e07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/scannet/scene0011_00'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scene_fold_from_img_path(img_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b5d7eb6-2a99-4aa2-b365-b7aa5876a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scene_fold_from_img_path(img_path):\n",
    "    return os.path.join(*(img_path.split('/')[:-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8b70ab-1d87-4309-87a0-08ae3dee9fb9",
   "metadata": {},
   "source": [
    "## get_scene_fold_from_img_path(img_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2e7d98e-3c6f-4ab6-a992-b2636bdab126",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'config_file' : '/home/chengshun.wang/pjs/mmsegmentation/m2f_pt/mask2former_r50_scannet_2d_240x320_pretrain.py',\n",
    "  'checkpoint_file' : '/home/chengshun.wang/pjs/mmsegmentation/m2f_pt/best_mIoU_iter_85000.pth',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e41eb9e3-f878-4ce3-8c6a-dad51882cd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: /home/chengshun.wang/pjs/mmsegmentation/m2f_pt/best_mIoU_iter_85000.pth\n"
     ]
    }
   ],
   "source": [
    "model = get_m2f(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b912d8e-430e-4c5d-a5ff-8d36ade7750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(downsample_gaussian_blur(\n",
    "        img, 320/1296), (240, 320), interpolation=cv2.INTER_LINEAR)\n",
    "    return img\n",
    "def downsample_gaussian_blur(img, ratio):\n",
    "    sigma = (1 / ratio) / 3\n",
    "    # ksize=np.ceil(2*sigma)\n",
    "    ksize = int(np.ceil(((sigma - 0.8) / 0.3 + 1) * 2 + 1))\n",
    "    ksize = ksize + 1 if ksize % 2 == 0 else ksize\n",
    "    img = cv2.GaussianBlur(img, (ksize, ksize), sigma, borderType=cv2.BORDER_REFLECT101)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90df471a-9609-4800-86b6-1b0b51fa851e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 44.35 GiB total capacity; 3.12 GiB already allocated; 22.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m img_id \u001b[38;5;241m=\u001b[39m get_img_id_from_img_path(i)\n\u001b[1;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(scene_dir,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm2f\u001b[39m\u001b[38;5;124m'\u001b[39m),exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m seg_logits, pred_sem_seg, mlvl_feats \u001b[38;5;241m=\u001b[39m \u001b[43mget_m2f_inference_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43mget_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m seg_logits \u001b[38;5;241m=\u001b[39m seg_logits[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m      7\u001b[0m pred_sem_seg  \u001b[38;5;241m=\u001b[39mpred_sem_seg[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/pjs/Semantic-Ray/sray/network/mask2former/mask2former.py:26\u001b[0m, in \u001b[0;36mget_m2f_inference_outputs\u001b[0;34m(model, imgs)\u001b[0m\n\u001b[1;32m     23\u001b[0m hook \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecode_head\u001b[38;5;241m.\u001b[39mpixel_decoder\u001b[38;5;241m.\u001b[39mregister_forward_hook(forward_hook) \n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# imgs = imgs.split(imgs.shape[0])\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# imgs = [i.cpu().numpy() for i in imgs]\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43minference_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m hook\u001b[38;5;241m.\u001b[39mremove() \n\u001b[1;32m     28\u001b[0m seg_logits \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/pjs/mmsegmentation/mmseg/apis/inference.py:111\u001b[0m, in \u001b[0;36minference_model\u001b[0;34m(model, img)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# forward the model\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 111\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m is_batch \u001b[38;5;28;01melse\u001b[39;00m results[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/semray/lib/python3.10/site-packages/mmengine/model/base_model/base_model.py:181\u001b[0m, in \u001b[0;36mBaseModel.test_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"``BaseModel`` implements ``test_step`` the same as ``val_step``.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    list: The predictions of given data.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    180\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_preprocessor(data, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/semray/lib/python3.10/site-packages/mmengine/model/base_model/base_model.py:376\u001b[0m, in \u001b[0;36mBaseModel._run_forward\u001b[0;34m(self, data, mode)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Unpacks data for :meth:`forward`\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    dict or list: Results of training or testing mode.\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 376\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    378\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39mdata, mode\u001b[38;5;241m=\u001b[39mmode)\n",
      "File \u001b[0;32m~/miniconda3/envs/semray/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/pjs/mmsegmentation/mmseg/models/segmentors/base.py:96\u001b[0m, in \u001b[0;36mBaseSegmentor.forward\u001b[0;34m(self, inputs, data_samples, mode)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(inputs, data_samples)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensor\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(inputs, data_samples)\n",
      "File \u001b[0;32m~/pjs/mmsegmentation/mmseg/models/segmentors/encoder_decoder.py:220\u001b[0m, in \u001b[0;36mEncoderDecoder.predict\u001b[0;34m(self, inputs, data_samples)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     batch_img_metas \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    214\u001b[0m             ori_shape\u001b[38;5;241m=\u001b[39minputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m             padding_size\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    218\u001b[0m     ] \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 220\u001b[0m seg_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_img_metas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess_result(seg_logits, data_samples)\n",
      "File \u001b[0;32m~/pjs/mmsegmentation/mmseg/models/segmentors/encoder_decoder.py:345\u001b[0m, in \u001b[0;36mEncoderDecoder.inference\u001b[0;34m(self, inputs, batch_img_metas)\u001b[0m\n\u001b[1;32m    343\u001b[0m     seg_logit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslide_inference(inputs, batch_img_metas)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m     seg_logit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhole_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_img_metas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m seg_logit\n",
      "File \u001b[0;32m~/pjs/mmsegmentation/mmseg/models/segmentors/encoder_decoder.py:314\u001b[0m, in \u001b[0;36mEncoderDecoder.whole_inference\u001b[0;34m(self, inputs, batch_img_metas)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwhole_inference\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Tensor,\n\u001b[1;32m    297\u001b[0m                     batch_img_metas: List[\u001b[38;5;28mdict\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    298\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Inference with full image.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \n\u001b[1;32m    300\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03m            input image.\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     seg_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_img_metas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m seg_logits\n",
      "File \u001b[0;32m~/pjs/mmsegmentation/mmseg/models/segmentors/encoder_decoder.py:129\u001b[0m, in \u001b[0;36mEncoderDecoder.encode_decode\u001b[0;34m(self, inputs, batch_img_metas)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Encode images with backbone and decode into a semantic segmentation\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03mmap of the same size as input.\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_feat(inputs)\n\u001b[0;32m--> 129\u001b[0m seg_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_img_metas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m                                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m seg_logits\n",
      "File \u001b[0;32m~/pjs/mmsegmentation/mmseg/models/decode_heads/mask2former_head.py:162\u001b[0m, in \u001b[0;36mMask2FormerHead.predict\u001b[0;34m(self, x, batch_img_metas, test_cfg)\u001b[0m\n\u001b[1;32m    160\u001b[0m cls_score \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(mask_cls_results, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    161\u001b[0m mask_pred \u001b[38;5;241m=\u001b[39m mask_pred_results\u001b[38;5;241m.\u001b[39msigmoid()\n\u001b[0;32m--> 162\u001b[0m seg_logits \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbqc, bqhw->bchw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m seg_logits\n",
      "File \u001b[0;32m~/miniconda3/envs/semray/lib/python3.10/site-packages/torch/functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    380\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 44.35 GiB total capacity; 3.12 GiB already allocated; 22.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for i in img_files:\n",
    "    scene_dir = get_scene_fold_from_img_path(i)\n",
    "    img_id = get_img_id_from_img_path(i)\n",
    "    os.makedirs(os.path.join(scene_dir,'m2f'),exist_ok=True)\n",
    "    seg_logits, pred_sem_seg, mlvl_feats = get_m2f_inference_outputs(model,[get_img(i)])\n",
    "    seg_logits = seg_logits[0].cpu()\n",
    "    pred_sem_seg  =pred_sem_seg[0].cpu()\n",
    "    mlvl_feats = torch.stack(mlvl_feats,1)[0].cpu()\n",
    "    # print(seg_logits.shape)\n",
    "    # print(pred_sem_seg.shape)\n",
    "    # print(mlvl_feats.shape)\n",
    "    ret=dict(seg_logits=seg_logits, pred_sem_seg = pred_sem_seg, mlvl_feats = mlvl_feats)\n",
    "    torch.save(ret,os.path.join(scene_dir,'m2f',f'm2f_{img_id}.pt'))\n",
    "    # # torch.save(ret,os.path.join('m2f.pt'))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04efcd14-1428-48f5-aec7-5a85470f6fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load('m2f.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9426eb1-68b4-4a8a-93cd-2c457bf3cf4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9468692-5734-493f-9f2d-0814a9fe44c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa88f4b-b9d8-458b-9fd6-03f34aa9cf1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3971a322-37bb-4014-a202-fc13d2124470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bca38c-991c-4b47-b12a-86351ba9d97a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c6565-ef5e-4dfa-a47f-c98ef276e114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7959c771-7b14-4dd9-9900-50cfff4c793a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e3f9a9-8ede-4516-89cf-bbc64f64bd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
